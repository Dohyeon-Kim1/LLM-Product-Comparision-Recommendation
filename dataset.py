import os
import pandas as pd
from tqdm import tqdm
from openai import OpenAI

from models import generation_pipe
from utils import webSearch, llmCompression


API = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))


SYSTEM_PROMPT = """
You are a helpful assistant that generates accurate answer on query based on context.
Answer in Korean as much as possible.
ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ 'ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë³´ë‹¤ ìƒì„¸í•œ ë¹„êµë¥¼ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ™‚'ë¼ëŠ” ë§ì„ ë¶™ì—¬ì£¼ì„¸ìš”.
""".strip()


INSTRUCTION = """
ì•„ë˜ì˜ ê´€ë ¨ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ê´€ë ¨ ì •ë³´ì— ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì´ ì—†ë‹¤ë©´ **ì œê³µëœ ì •ë³´ê°€ ì—†ë‹¤ëŠ” ë§ì€ ì œì™¸í•˜ê³ ** í˜„ì¬ ì•Œê³  ìˆëŠ” ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
""".strip()


USER_PROMPT = """
{instruction}

ì§ˆë¬¸:
{query}

ê´€ë ¨ ì •ë³´:
{context}
""".strip()


def main():
    # amazon ìƒí’ˆ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    df = pd.read_csv("hf://datasets/bprateek/amazon_product_description/marketing_sample_for_amazon_com-ecommerce__20200101_20200131__10k_data.csv")
    df = df.dropna(axis=1, how='all')

    # amazon ìƒí’ˆ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
    selected_columns = [
        'Product Name',
        'Category',
        'Selling Price',
        'About Product',
        'Product Specification',
        'Technical Details',
        'Product Dimensions',
        'Shipping Weight'
    ]
    selected_df = df[selected_columns]
    df_filtered = selected_df.dropna(subset=['Selling Price'])

    # ì¿¼ë¦¬ ìƒì„±
    queries = []
    for _, row in df_filtered.iterrows():
        query = f"{row['Product Name']}ê³¼ ë¹„ìŠ·í•œ ê°€ê²©ëŒ€ì˜ ì œí’ˆì„ ì¶”ì²œí•˜ê³  ë¹„êµí•´ì£¼ì„¸ìš”."

        queries.append({
            'product_name' : row['Product Name'],
            'query': query
        })

    # context ì••ì¶• íŒŒì´í”„ë¼ì¸ ë¶ˆëŸ¬ì˜¤ê¸°
    pipe = generation_pipe("google/gemma-2-9b-it")

    # ì§ˆë¬¸-ì •ë³´-ë‹µë³€ ë°ì´í„°ì…‹ êµ¬ì¶•
    rows = []
    for query in tqdm(queries):
        raw_context = webSearch(query)
        context = llmCompression(pipe, raw_context)        
        user_prompt = USER_PROMPT.format(
            instruction=INSTRUCTION,
            query=query,
            context=context
        )

        response = API.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        ).choices[0].message.content

        row = {
            "system": SYSTEM_PROMPT,
            "instruction": INSTRUCTION,
            "query": query,
            "context": context,
            "response": response
        }
        rows.append(row)
    
    dataset = pd.DataFrame(rows)
    dataset.to_csv("./dataset.csv", index=False)


if __name__ == "__main__":
    main()

