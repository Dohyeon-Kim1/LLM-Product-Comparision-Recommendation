import wandb
from datasets import load_dataset
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq


USER_PROMPT = """
<start_of_turn>user
You are an expert product recommendation AI assistant trained to:
1. Compare products objectively based on features and specifications.
2. Provide personalized recommendations considering user needs.
3. Explain the pros and cons of each product clearly.
4. Support recommendations with factual information.

[System]
ë‹¹ì‹ ì€ ì œí’ˆ ì¶”ì²œê³¼ ë¹„êµë¥¼ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.
ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ 'ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë³´ë‹¤ ìƒì„¸í•œ ë¹„êµë¥¼ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ™‚'ë¼ëŠ” ë§ì„ ë¶™ì—¬ì£¼ì„¸ìš”.

[Instruction]
ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë§Œì•½ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ì—†ë”ë¼ë„, ê·¸ ì‚¬ì‹¤ì„ ì–¸ê¸‰í•˜ì§€ ë§ê³  í˜„ì¬ ë³´ìœ í•œ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

[Question]
{query}

[Context]
ì„¤ëª…: {context}
<end_of_turn>
""".strip()


MODEL_RESPONSE = """
<start_of_turn>model
{response}
<end_of_turn>
""".strip()


def train(model, tokenizer, args):
    # wandb ì´ˆê¸°í™”
    wandb.init(
        project = "Product Comparision & Recommendataion",
        config = {
            "model": "google/gemma-2-9b-it",
            "learning_rate": args.lr,
            "epochs": args.epoch,
            "batch_size": args.batch_size
        }
    )

    # ë°ì´í„° ì „ì²˜ë¦¬*í”„ë¡¬í”„íŠ¸ í¬ë§· í•¨ìˆ˜ ì •ì˜
    def formatting_func(example, add_bos=True, add_eos=True):
        try:
            # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦(ë°ì´í„°ì…‹ì˜ í•„ìˆ˜ í•­ëª©ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ê°€)
            required_fields = ['system', 'instruction', 'query', 'context', 'response']
            for field in required_fields:
                if field not in example or not example[field]:
                    raise ValueError(f"Missing or empty required field: {field}")

            # 2. í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜(ì…ë ¥ í…ìŠ¤íŠ¸ ì •ë¦¬)
            def clean_text(text):
                return text.strip().replace('\n', ' ').replace('  ', ' ')  #strip(): ì•ë’¤ ê³µë°± ì œê±°
                                                                        #replace('\n', ' '): ì¤„ë°”ê¿ˆ -> ê³µë°± ë³€í™˜
                                                                        #replace('  ', ' '): ë‘ ê°œ ì´ìƒ ì—°ì† ê³µë°± í•˜ë‚˜ë¡œ ì¤„ì„

            # 3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…(aiì˜ ì—­í• ê³¼ ì‘ì—… ì •ì˜í•´ì¤Œ)
            # ì¡°ê±´ë¶€ í˜•ì‹ ì‚¬ìš©í•´ì„œ ì²˜ë¦¬(bos: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‹œì‘ ë¶€ë¶„/eos: ëª¨ë¸ ì‘ë‹µ ë ë¶€ë¶„)
            # 4. ì‚¬ìš©ì ì…ë ¥ í¬ë§·íŒ…(ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ êµ¬ì¡°í™”)
            user_query = USER_PROMPT.format(
                query=clean_text(example["query"]), 
                context=clean_text(example["context"])
            ) 
            user_query = f"<bos>{user_query}" if add_bos else user_query

            # 5. ëª¨ë¸ ì‘ë‹µ í¬ë§·íŒ…(aiì˜ ì‘ë‹µ í¬ë§· ì •ì˜)
            model_response = MODEL_RESPONSE.format(response=example["response"])
            model_response = f"{model_response}<eos>" if add_eos else model_response

            # 6. ì „ì²´ í”„ë¡¬í”„íŠ¸ ì¡°í•©(ê° ì„¹ì…˜ì„ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ í†µí•©)
            prompt = f"{user_query}\n{model_response}"
            return prompt

        #ì—ëŸ¬ì²˜ë¦¬
        except Exception as e:
            print(f"Error in formatting prompt: {str(e)}")
            return None  #ì—ëŸ¬ ë°œìƒ ì‹œ none ë°˜í™˜

    # ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
    def preprocess_function(example): #data ëŒ€ì‹  exampleì„ ë°›ìŒ
        #ë‹¨ì¼ ì˜ˆì œì— ëŒ€í•´ formatting_func ì ìš©
        text = formatting_func(example)

        encoded = tokenizer(
            text,
            truncation=True,
            max_length=2048,
            padding=True,
            add_special_tokens=False
        )

        labels = encoded["input_ids"].copy()
        response_start = text.find("<start_of_turn>model")
        if response_start != -1:
            response_start_tokens = tokenizer(
                text[:response_start],
                add_special_tokens=False,
                truncation=True,
                max_length=2048
            )
            label_mask = [i < len(response_start_tokens["input_ids"]) for i in range(len(labels))]
            labels = [-100 if mask else lab for mask, lab in zip(label_mask, labels)]

        encoded["labels"] = labels
        return encoded

    # ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    dataset = load_dataset(args.dataset, split="train")

    #ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì ìš©
    tokenized_dataset = dataset.map(
        preprocess_function,
        remove_columns=dataset.column_names,
        num_proc=4
    )

    # ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        padding=True,
        return_tensors="pt"
    )

    # í•™ìŠµíŒŒë¼ë¯¸í„° ì„¤ì •
    training_args = TrainingArguments(
        output_dir= f"{args.output_dir}/checkpoints",
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size // args.gradient_accumulation_steps,
        learning_rate=args.lr,
        save_strategy="epoch",
        logging_dir="./logs",
        report_to="wandb",
        warmup_ratio=args.warmup_ratio,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        fp16=True,
        fp16_opt_level="O2",
        gradient_checkpointing=True,
        lr_scheduler_type=args.lr_schedular,
    )

    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = Trainer(
        model=model, # íŒŒì¸íŠœë‹í•  ëª¨ë¸
        args=training_args, # í•™ìŠµ íŒŒë¼ë¯¸í„°
        train_dataset=tokenized_dataset, # í•™ìŠµ ë°ì´í„°ì…‹
        data_collator=data_collator, # ë°ì´í„° ì½œë ˆì´í„°
        tokenizer=tokenizer
    )

    # ëª¨ë¸ í•™ìŠµ
    trainer.train()
    
    # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    model.save_pretrained_merged(
        f"{args.output_dir}/checkpoints/last",
        tokenizer,
        save_method="merged_16bit"
    )